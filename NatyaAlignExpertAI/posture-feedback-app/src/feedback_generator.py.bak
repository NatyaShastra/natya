from mediapipe import solutions
import cv2
import numpy as np
import pickle
import os
from collections import Counter
import textwrap
from dance_steps import get_step_id_from_name
from ffmpeg_utils import cv2_frames_to_ffmpeg_video
import subprocess

class FeedbackGenerator:
    def annotate_video_with_feedback(self, video_path, output_path, specified_step=None, frame_sample_rate=3):
        """
        Create an annotated video with feedback overlaid.
        
        Args:
            video_path: Path to the input video
            output_path: Path to save the output video
            specified_step: Optional dance step name
            frame_sample_rate: Only process 1 out of every N frames (default: 3)
                               Higher values = faster processing but lower quality
        
        Returns:
            Path to the annotated video if successful, None otherwise
            List of feedback items
        """
        # Store all processed frames here in case we need to use ffmpeg fallback
        all_processed_frames = []
        
        try:
            cap = cv2.VideoCapture(video_path)
            if not cap.isOpened():
                print(f"Error: Could not open video file: {video_path}")
                return None, []
                
    def _finalize_with_ffmpeg(self, video_path):
        """
        Ensure a video uses H.264 codec for maximum browser compatibility.
        
        Args:
            video_path: Path to the video file
            
        Returns:
            Path to the finalized video (same as input if successful)
        """
        try:
            # First check the current codec
            cmd = [
                'ffprobe', '-v', 'error',
                '-select_streams', 'v:0',
                '-show_entries', 'stream=codec_name',
                '-of', 'default=noprint_wrappers=1:nokey=1',
                video_path
            ]
            
            result = subprocess.run(cmd, capture_output=True, text=True)
            current_codec = result.stdout.strip()
            
            # If already H.264, no need to convert
            if current_codec == 'h264' or current_codec == 'avc1':
                print(f"Video already uses H.264 codec: {video_path}")
                return video_path
                
            print(f"Current codec is {current_codec}, converting to H.264...")
            
            # Create a temporary file for the new video
            temp_output = f"{video_path}.h264.mp4"
            
            # Convert to H.264
            cmd = [
                'ffmpeg', '-y',
                '-i', video_path,
                '-c:v', 'libx264',
                '-preset', 'fast',
                '-crf', '23',
                '-pix_fmt', 'yuv420p',
                temp_output
            ]
            
            result = subprocess.run(cmd, capture_output=True, text=True)
            if result.returncode != 0:
                print(f"Error converting video to H.264: {result.stderr}")
                return video_path
                
            # Replace the original video with the H.264 version
            os.replace(temp_output, video_path)
            print(f"Successfully converted video to H.264: {video_path}")
            
            return video_path
            
        except Exception as e:
            print(f"Error finalizing video with ffmpeg: {str(e)}")
            return video_path
    
    def _create_video_with_ffmpeg_fallback(self, frames, output_path, fps=30):
        """
        Create a video using ffmpeg as a fallback method.
        
        Args:
            frames: List of frames as numpy arrays
            output_path: Path to save the output video
            fps: Frames per second
            
        Returns:
            Path to the created video or None if failed
        """
        try:
            if not frames:
                print("No frames to create video")
                return None
                
            print(f"Creating video with ffmpeg fallback: {output_path}")
            
            # Use our utility function to create the video with ffmpeg
            if cv2_frames_to_ffmpeg_video(frames, output_path, fps):
                print(f"Successfully created video with ffmpeg: {output_path}")
                return output_path
            else:
                print(f"Failed to create video with ffmpeg: {output_path}")
                return None
                
        except Exception as e:
            print(f"Error in _create_video_with_ffmpeg_fallback: {str(e)}")
            return None
    
    def annotate_video_with_feedback(self, video_path, output_path, specified_step=None, frame_sample_rate=3):
        """
        Create an annotated video with feedback overlaid.
        
        Args:
            video_path: Path to the input video
            output_path: Path to save the output video
            specified_step: Optional dance step name
            frame_sample_rate: Only process 1 out of every N frames (default: 3)
                              Higher values = faster processing but lower quality
        
        Returns:
            Path to the annotated video if successful, None otherwise
            List of feedback items
        """
        # Store all processed frames here in case we need to use ffmpeg fallback
        all_processed_frames = []
        
        try:
            cap = cv2.VideoCapture(video_path)
            if not cap.isOpened():
                print(f"Error: Could not open video file: {video_path}")
                return None, []
                
            # Try H.264 codec first (most browser compatible)
            fourcc = cv2.VideoWriter_fourcc(*'avc1')  # H.264 codec
            fps = int(cap.get(cv2.CAP_PROP_FPS))
            width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
            height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
            
            # Use default values if we can't determine the video properties
            if width <= 0 or height <= 0:
                print(f"Warning: Invalid video dimensions. Using defaults.")
                width = 640
                height = 480
            
            if fps <= 0:
                print(f"Warning: Invalid FPS. Using default.")
                fps = 30
            
            # Print codec information for debugging
            print(f"Trying codec: avc1 (H.264) for video: {output_path}")
            print(f"Video dimensions: {width}x{height}, FPS: {fps}")
            print(f"Frame sample rate: {frame_sample_rate} (processing 1 out of every {frame_sample_rate} frames)")
            
            # Create the output directory if it doesn't exist
            output_dir = os.path.dirname(output_path)
            if not os.path.exists(output_dir):
                os.makedirs(output_dir)
            
            # Flag to track if OpenCV video writing is working
            use_opencv = True
            
            try:
                # Try to create VideoWriter with H.264 codec
                out = cv2.VideoWriter(output_path, fourcc, fps, (width, height))
                
                # Verify the video writer was initialized properly
                if not out.isOpened():
                    print(f"H.264 codec failed. Trying fallback to mp4v codec...")
                    fourcc = cv2.VideoWriter_fourcc(*'mp4v')
                    out = cv2.VideoWriter(output_path, fourcc, fps, (width, height))
                    
                    if not out.isOpened():
                        print(f"mp4v codec failed. Trying fallback to XVID codec...")
                        fourcc = cv2.VideoWriter_fourcc(*'XVID')
                        out = cv2.VideoWriter(output_path, fourcc, fps, (width, height))
                        
                        if not out.isOpened():
                            print(f"All OpenCV codecs failed. Will use ffmpeg directly.")
                            use_opencv = False
                        else:
                            print(f"Using fallback codec: XVID for video: {output_path}")
                    else:
                        print(f"Using fallback codec: mp4v for video: {output_path}")
                else:
                    print(f"Successfully initialized H.264 codec for video: {output_path}")
            except Exception as e:
                print(f"Error creating VideoWriter: {e}")
                use_opencv = False
                
        except Exception as e:
            print(f"Error initializing video processing: {e}")
            return None, []

        # Extract all landmarks first
        frames = []
        landmarks_list = []
        frame_count = 0
        
        print(f"Extracting frames and landmarks (sampling 1 out of every {frame_sample_rate} frames)...")
        start_time = subprocess.check_output("date +%s", shell=True).decode().strip()
        
        while cap.isOpened():
            ret, frame = cap.read()
            if not ret:
                break
                
            # Only process every Nth frame based on the sample rate
            # Always process the first frame
            if frame_count == 0 or frame_count % frame_sample_rate == 0:
                frames.append(frame)
                results = self.mp_pose.process(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))
                
                if results.pose_landmarks:
                    landmarks = self._extract_landmarks(results.pose_landmarks)
                    landmarks_list.append(landmarks)
            
            frame_count += 1
        
        cap.release()
        
        end_time = subprocess.check_output("date +%s", shell=True).decode().strip()
        processing_time = int(end_time) - int(start_time)
        print(f"Extracted {len(frames)} frames and {len(landmarks_list)} landmarks in {processing_time} seconds")
        print(f"Original video had {frame_count} frames, sampled down to {len(frames)} frames")
        
        if not landmarks_list:
            # No poses detected
            print("No poses detected in the video")
            if use_opencv:
                for frame in frames:
                    out.write(frame)
                out.release()
            return output_path, ["No pose detected in the video"]
            
        # Get dance step - either specified by user or identified automatically
        landmarks_array = np.array(landmarks_list)
        if specified_step:
            identified_step = specified_step
            confidence = 1.0  # User-specified, so confidence is high
            step_display = f"Dance step: {identified_step} (user specified)"
        else:
            identified_step, confidence = self.identify_dance_step(landmarks_array)
            step_display = f"Identified dance step: {identified_step} (confidence: {confidence:.2f})"
        
        print(f"Dance step identification: {step_display}")
        
        # Process each frame and annotate
        all_feedbacks = []
        
        # For optimization, calculate all possible feedbacks ahead of time
        print("Precomputing feedbacks for optimization...")
        start_time = subprocess.check_output("date +%s", shell=True).decode().strip()
        precomputed_feedbacks = {}
        try:
            if self.step_map and identified_step in self.step_map:
                print(f"Using step_map optimization for '{identified_step}'")
                step_id_num = self.step_map.get(identified_step, 0)
                
                # Calculate aggregate features from all landmarks at once
                all_landmarks_array = np.array(landmarks_list)
                feat_mean = np.nanmean(all_landmarks_array, axis=0)
                feat_std = np.nanstd(all_landmarks_array, axis=0)
                
                # Use 99-dimensional features as expected by the model
                if feat_mean.shape[0] != 99:
                    # Pad or truncate to 99 features if needed
                    feat_mean = np.pad(feat_mean, (0, max(0, 99 - feat_mean.shape[0])), 'constant')[:99]
                    feat_std = np.pad(feat_std, (0, max(0, 99 - feat_std.shape[0])), 'constant')[:99]
                    
                # Combine features
                feat = np.concatenate([feat_mean, feat_std])
                
                # Add the step_id feature
                feat_reshaped = feat.reshape(1, -1)
                step_id_array = np.array([[step_id_num]], dtype=np.float32)
                test_feat = np.hstack([feat_reshaped, step_id_array])
                
                # Get the global feedback for the entire video
                prediction = self.model.predict(test_feat)
                global_feedback = self.label_encoder.inverse_transform(prediction)[0] if self.label_encoder else prediction[0]
                precomputed_feedbacks['global'] = global_feedback
                print(f"Generated global feedback: '{global_feedback}'")
            else:
                # For models without step map, precompute key frame feedbacks
                print("Using key frame optimization")
                # Use a larger sampling interval for precomputation to save time
                precompute_interval = max(5, frame_sample_rate * 2)  # Skip even more frames for precomputation
                key_frames = list(range(0, len(landmarks_list), precompute_interval))
                
                for i in key_frames:
                    if i < len(landmarks_list):  # Safety check
                        try:
                            landmarks = np.array(landmarks_list[i]).reshape(1, -1)
                            prediction = self.model.predict(landmarks)
                            correction = self.label_encoder.inverse_transform(prediction)[0] if self.label_encoder else prediction[0]
                            precomputed_feedbacks[i] = correction
                        except Exception as e:
                            print(f"Error in precomputing feedback for frame {i}: {e}")
                
                print(f"Precomputed feedbacks for {len(precomputed_feedbacks)} key frames")
        except Exception as e:
            print(f"Error in precomputing feedbacks: {e}")
        
        end_time = subprocess.check_output("date +%s", shell=True).decode().strip()
        processing_time = int(end_time) - int(start_time)
        print(f"Precomputing feedbacks completed in {processing_time} seconds")
        
        # Now process frames for display
        print("Generating annotated video frames...")
        start_time = subprocess.check_output("date +%s", shell=True).decode().strip()
        frame_count = 0
        
        for i, frame in enumerate(frames):
            # Progress tracking
            if i % 10 == 0:
                print(f"Processing frame {i}/{len(frames)} ({(i/len(frames)*100):.1f}%)")
            
            # Process pose landmarks for visualization
            results = self.mp_pose.process(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))
            if results.pose_landmarks:
                # Draw pose landmarks
                mp_drawing = solutions.drawing_utils
                mp_drawing.draw_landmarks(frame, results.pose_landmarks, solutions.pose.POSE_CONNECTIONS)
                
                # Use precomputed feedback instead of calculating for every frame
                if precomputed_feedbacks:
                    if 'global' in precomputed_feedbacks:
                        # Use global feedback for all frames
                        correction_tip = precomputed_feedbacks['global']
                    elif i in precomputed_feedbacks:
                        # Use the precomputed feedback for this frame
                        correction_tip = precomputed_feedbacks[i]
                    else:
                        # Find the closest precomputed frame
                        closest_key = min(precomputed_feedbacks.keys(), key=lambda k: abs(k-i)) if precomputed_feedbacks else None
                        correction_tip = precomputed_feedbacks.get(closest_key, "Analyzing...")
                else:
                    # Fallback to simple analysis for this frame only
                    try:
                        landmark_idx = min(i, len(landmarks_list)-1) if landmarks_list else -1
                        if landmark_idx >= 0:
                            landmarks = np.array(landmarks_list[landmark_idx]).reshape(1, -1)
                            prediction = self.model.predict(landmarks)
                            correction_tip = self.label_encoder.inverse_transform(prediction)[0] if self.label_encoder else prediction[0]
                        else:
                            correction_tip = "No pose detected"
                    except Exception as e:
                        print(f"Error in simple annotation: {e}")
                        correction_tip = "Analysis error"
                
                all_feedbacks.append(correction_tip)
                
                # Add the feedback as text overlay on the frame
                cv2.putText(frame, f"Feedback: {correction_tip}", (10, 30), 
                            cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 0, 255), 2, cv2.LINE_AA)
            
            # Write the annotated frame
            if use_opencv:
                out.write(frame)
            all_processed_frames.append(frame.copy())  # Store for possible ffmpeg fallback
            frame_count += 1
        
        end_time = subprocess.check_output("date +%s", shell=True).decode().strip()
        processing_time = int(end_time) - int(start_time)
        print(f"Generated {frame_count} annotated frames in {processing_time} seconds")

        # Find top 3 most frequent actionable corrections
        print("Generating feedback summary...")
        feedback_counts = Counter(all_feedbacks)
        
        # Only actionable corrections (negative feedback)
        def is_correction(fb):
            fb_lower = fb.lower()
            return (
                "not" in fb_lower or "imperfect" in fb_lower or "bad" in fb_lower or 
                "incorrect" in fb_lower or "zero" in fb_lower or "shaking" in fb_lower or 
                "wrong" in fb_lower
            )
            
        corrections = [fb for fb, _ in feedback_counts.most_common() if is_correction(fb)]
        top_corrections = corrections[:3]
        
        print(f"Found {len(corrections)} corrections, using top {len(top_corrections)} for summary")

        # We'll keep track of all the feedback but won't add it to the video
        # Just use this for return values and the user-friendly summary
        feedback_summary = []
        
        # Add dance step identification to the feedback summary
        feedback_summary.append(step_display)
        
        # Add the top corrections to the feedback summary
        if top_corrections:
            for correction in top_corrections:
                feedback_summary.append(correction)
        else:
            feedback_summary.append("No major corrections detected.")
        
        print(f"Feedback summary: {feedback_summary}")
            
        # Calculate how many frames we need to add to reach minimum 10 seconds
        total_frames_so_far = len(frames)  # only video frames, no feedback frame
        total_seconds_so_far = total_frames_so_far / fps
        
        print(f"Video duration so far: {total_seconds_so_far:.2f} seconds")
        
        # Ensure at least 10 seconds total duration
        min_seconds = 10
        if total_seconds_so_far < min_seconds:
            # For the remaining duration, just repeat the last frame
            additional_seconds = min_seconds - total_seconds_so_far
            additional_frames = int(additional_seconds * fps)
            print(f"Adding {additional_frames} additional frames to reach minimum {min_seconds} seconds")
            
            # Get the last frame to repeat
            last_frame = frames[-1].copy() if frames else np.ones((height, width, 3), dtype=np.uint8) * 255
            
            # Add last frame copies to meet the minimum duration
            print("Adding additional frames...")
            for _ in range(additional_frames):
                if use_opencv:
                    out.write(last_frame)
                all_processed_frames.append(last_frame.copy())

        # Try the main method of creating the video
        print("Finalizing video...")
        start_time = subprocess.check_output("date +%s", shell=True).decode().strip()
        
        success = False
        if use_opencv:
            try:
                out.release()
                
                # Verify the output file was created and has a non-zero size
                if os.path.exists(output_path) and os.path.getsize(output_path) > 0:
                    print(f"Successfully created video with OpenCV: {output_path}")
                    success = True
                else:
                    print(f"OpenCV claimed success but video file not created properly")
            except Exception as e:
                print(f"Error finalizing OpenCV video: {e}")
                
        # If OpenCV method failed or was not used, try with ffmpeg
        if not success and all_processed_frames:
            print(f"Using ffmpeg fallback to create video: {output_path}")
            try:
                success = cv2_frames_to_ffmpeg_video(all_processed_frames, output_path, fps)
                if success:
                    print(f"Successfully created video with ffmpeg: {output_path}")
                else:
                    print(f"Failed to create video with ffmpeg: {output_path}")
            except Exception as e:
                print(f"Error using ffmpeg: {e}")
            
        # If the video was created successfully, ensure it meets minimum duration
        if success and os.path.exists(output_path):
            print("Ensuring minimum video duration...")
            try:
                from ffmpeg_utils import ensure_min_duration
                ensure_min_duration(output_path, min_duration=10.0)
                print(f"Successfully ensured minimum duration of 10 seconds")
            except Exception as e:
                print(f"Error ensuring minimum duration: {e}")
                # Continue anyway, as we at least have the video
            
            # Always finalize with ffmpeg to ensure H.264 codec
            print("Finalizing video with ffmpeg to ensure H.264 codec...")
            try:
                finalized_path = self._finalize_with_ffmpeg(output_path)
                
                # Check total processing time
                end_time = subprocess.check_output("date +%s", shell=True).decode().strip()
                processing_time = int(end_time) - int(start_time)
                print(f"Video finalization completed in {processing_time} seconds")
                
                # Return the feedback_summary along with the video path
                print(f"Video creation complete: {finalized_path}")
                return finalized_path, feedback_summary
            except Exception as e:
                print(f"Error finalizing with ffmpeg: {e}")
                # Return the original path if ffmpeg finalization failed
                return output_path, feedback_summary
        else:
            print(f"Failed to create video: {output_path}")
            return None, []
    def __init__(self, model_path):
        with open(model_path, 'rb') as model_file:
            model_data = pickle.load(model_file)
            # Handle both formats: (clf, label_encoder) or (clf, label_encoder, step_map)
            if len(model_data) == 3:
                self.model, self.label_encoder, self.step_map = model_data
                # Reverse the step map for lookup
                self.id_to_step = {v: k for k, v in self.step_map.items()} if self.step_map else {}
            else:
                self.model, self.label_encoder = model_data
                self.step_map = None
                self.id_to_step = {}
                
        self.mp_pose = solutions.pose.Pose(
            static_image_mode=False, 
            model_complexity=2, 
            enable_segmentation=True, 
            min_detection_confidence=0.5, 
            min_tracking_confidence=0.5
        )

    def analyze_posture(self, video_path):
        """Analyze the student's posture in a video"""
        cap = cv2.VideoCapture(video_path)
        frames_landmarks = []
        
        # Extract landmarks from all frames
        while cap.isOpened():
            ret, frame = cap.read()
            if not ret:
                break
                
            results = self.mp_pose.process(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))
            if results.pose_landmarks:
                landmarks = self._extract_landmarks(results.pose_landmarks)
                frames_landmarks.append(landmarks)
        
        cap.release()
        
        if not frames_landmarks:
            return ["No pose detected in video"]
        
        # Convert to numpy array
        landmarks_array = np.array(frames_landmarks)
        
        # Stage 1: Identify the dance step
        identified_step, confidence = self.identify_dance_step(landmarks_array)
        
        # Stage 2: Validate the posture using the identified step
        feedback = self.validate_posture(landmarks_array, identified_step)
        
        # Add step identification to feedback if available
        if self.step_map:
            feedback.insert(0, f"Identified dance step: {identified_step} (confidence: {confidence:.2f})")
        
        return feedback
        
    def analyze_posture_with_step(self, video_path, dance_step):
        """
        Analyze the student's posture in a video using a specified dance step
        
        Args:
            video_path: Path to the video file
            dance_step: Name of the dance step the student is performing
            
        Returns:
            List of feedback strings
        """
        cap = cv2.VideoCapture(video_path)
        frames_landmarks = []
        
        # Extract landmarks from all frames
        while cap.isOpened():
            ret, frame = cap.read()
            if not ret:
                break
                
            results = self.mp_pose.process(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))
            if results.pose_landmarks:
                landmarks = self._extract_landmarks(results.pose_landmarks)
                frames_landmarks.append(landmarks)
        
        cap.release()
        
        if not frames_landmarks:
            return ["No pose detected in video"]
        
        # Convert to numpy array
        landmarks_array = np.array(frames_landmarks)
        
        # Skip step identification and use the provided dance step
        # Validate the posture using the specified step
        feedback = self.validate_posture(landmarks_array, dance_step)
        
        # Add step information to feedback
        feedback.insert(0, f"Analyzing dance step: {dance_step} (user specified)")
        
        return feedback

    def _extract_landmarks(self, landmarks):
        """Extract landmarks from a MediaPipe pose landmarks object"""
        # Extract the raw landmarks
        landmarks_array = np.array([[lm.x, lm.y, lm.z] for lm in landmarks.landmark]).flatten()
        
        # Log the shape to debug
        # print(f"Extracted landmarks shape: {landmarks_array.shape}")
        
        return landmarks_array

    def identify_dance_step(self, landmarks_array):
        """First stage: Identify which dance step the student is performing"""
        if not self.step_map:
            return "unknown", 0.0
        
        try:
            # Process landmarks as done during training
            # First reshape if needed
            if landmarks_array.ndim == 3:
                T = landmarks_array.shape[0]
                flat = landmarks_array.reshape(T, -1)
            else:
                flat = landmarks_array
                
            # Compute mean and std across all frames
            feat_mean = np.nanmean(flat, axis=0)
            feat_std = np.nanstd(flat, axis=0)
            
            # Use 99-dimensional features as expected by the model
            if feat_mean.shape[0] != 99:
                # Pad or truncate to 99 features if needed
                feat_mean = np.pad(feat_mean, (0, max(0, 99 - feat_mean.shape[0])), 'constant')[:99]
                feat_std = np.pad(feat_std, (0, max(0, 99 - feat_std.shape[0])), 'constant')[:99]
                
            feat = np.concatenate([feat_mean, feat_std])
            feat_reshaped = feat.reshape(1, -1)  # Make feat a 2D array
            
            # Get all possible step_ids
            step_ids = list(self.step_map.values())
            
            # Create test features for each possible step_id
            step_predictions = []
            for step_id in step_ids:
                # Create a copy of features for each possible step_id
                step_id_array = np.array([[step_id]], dtype=np.float32)  # Make step_id a 2D array
                test_feat = np.hstack([feat_reshaped, step_id_array])
                
                # Get probability of "correct" classification for this step_id
                probs = self.model.predict_proba(test_feat)
                
                # Find probability for "correct" class if it exists
                if "correct" in self.model.classes_:
                    correct_idx = np.where(self.model.classes_ == "correct")[0][0]
                    correct_prob = probs[0][correct_idx]
                else:
                    # If no "correct" class, use the highest probability class
                    correct_prob = probs.max()
                    
                step_predictions.append((step_id, correct_prob))
            
            # Choose step_id with highest probability of being "correct"
            best_step_id, best_prob = max(step_predictions, key=lambda x: x[1])
            return self.id_to_step.get(best_step_id, "unknown"), best_prob
        except Exception as e:
            print(f"Error in identify_dance_step: {e}")
            return "unknown", 0.0
            
            # Find probability for "correct" class if it exists
            if "correct" in self.model.classes_:
                correct_idx = np.where(self.model.classes_ == "correct")[0][0]
                correct_prob = probs[0][correct_idx]
            else:
                # If no "correct" class, use the highest probability class
                correct_prob = probs.max()
                
            step_predictions.append((step_id, correct_prob))
        
        # Choose step_id with highest probability of being "correct"
        best_step_id, best_prob = max(step_predictions, key=lambda x: x[1])
        return self.id_to_step.get(best_step_id, "unknown"), best_prob

    def validate_posture(self, landmarks_array, step_id):
        """Second stage: Validate the posture for the identified step"""
        # If no step map, use simple prediction
        if not self.step_map:
            return self._simple_posture_analysis(landmarks_array)
        
        # Check if the step is in our model
        step_id_num = None
        if step_id in self.step_map:
            step_id_num = self.step_map.get(step_id)
        
        # If step not in model, use a generic analysis
        if step_id_num is None:
            # Use simple analysis but add a note about the step not being trained
            basic_feedback = self._simple_posture_analysis(landmarks_array)
            basic_feedback.append(f"Note: '{step_id}' is not in the trained model yet. Consider recording more examples and retraining for specific feedback.")
            return basic_feedback
        
        try:    
            # Process landmarks as done during training
            # First reshape if needed
            if landmarks_array.ndim == 3:
                T = landmarks_array.shape[0]
                flat = landmarks_array.reshape(T, -1)
            else:
                flat = landmarks_array
                
            # Compute mean and std across all frames
            feat_mean = np.nanmean(flat, axis=0)
            feat_std = np.nanstd(flat, axis=0)
            
            # Debug info
            print(f"Mean features shape: {feat_mean.shape}")
            print(f"Std features shape: {feat_std.shape}")
            
            # Use 99-dimensional features as expected by the model (33 pose landmarks with x,y,z coordinates)
            # This assumes the model was trained with 99-dimensional feature vectors for both mean and std
            if feat_mean.shape[0] != 99:
                # Pad or truncate to 99 features if needed
                feat_mean = np.pad(feat_mean, (0, max(0, 99 - feat_mean.shape[0])), 'constant')[:99]
                feat_std = np.pad(feat_std, (0, max(0, 99 - feat_std.shape[0])), 'constant')[:99]
            
            feat = np.concatenate([feat_mean, feat_std])
            
            # Debug info
            print(f"Combined features shape: {feat.shape}")
            
            # Add the step_id feature - reshape feat to ensure it's 2D
            feat_reshaped = feat.reshape(1, -1)  # Make feat a 2D array
            step_id_array = np.array([[step_id_num]], dtype=np.float32)  # Make step_id a 2D array
            test_feat = np.hstack([feat_reshaped, step_id_array])
            
            # Debug info
            print(f"Final test features shape: {test_feat.shape}")
            
            # Predict posture quality
            prediction = self.model.predict(test_feat)
            
            if self.label_encoder:
                correction_tip = self.label_encoder.inverse_transform(prediction)[0]
            else:
                correction_tip = prediction[0]
                
            return [correction_tip]
        except Exception as e:
            print(f"Error in validate_posture: {e}")
            return ["Error analyzing posture. Please try again."]
        
        if self.label_encoder:
            correction_tip = self.label_encoder.inverse_transform(prediction)[0]
        else:
            correction_tip = prediction[0]
            
        return [correction_tip]
    
    def _simple_posture_analysis(self, landmarks_array):
        """Legacy analysis for old model format"""
        results = []
        for landmarks in landmarks_array:
            prediction = self.model.predict([landmarks])
            if self.label_encoder:
                correction_tip = self.label_encoder.inverse_transform(prediction)[0]
            else:
                correction_tip = prediction[0]
            results.append(correction_tip)
        return results

    # No longer needed, feedback is now the correction tip from the model

def get_feedback(video_path):
    generator = FeedbackGenerator('/workspaces/natya/NatyaAlignExpertAI/posture-feedback-app/trained_model/model.pkl')
    return generator.analyze_posture(video_path)